{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import natsort\n",
    "from tensorflow.keras.optimizers import  Adam\n",
    "import nibabel as nib\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Include path to my modules\n",
    "MY_UTILS_PATH = \"../Modules/\"\n",
    "if not MY_UTILS_PATH in sys.path:\n",
    "    sys.path.append(MY_UTILS_PATH)\n",
    "\n",
    "# Import my modules\n",
    "import cs_models_mc as fsnet\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "channels = 24\n",
    "batch_size = 8\n",
    "weights_path = \"../Models/weights_wwnet_ikik_mc_r10.h5\"\n",
    "model_string = \"ikik\"\n",
    "test_path = \"../../Dataset/Test-R=10/\"\n",
    "results_path = \"../WWnet-ikik-baseline/Track01/12-channel-R=10/\"\n",
    "cascade = \"unet\"\n",
    "crop = (50,-50) # slices to crop\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet\n",
      "Domains:  ikik\n",
      "Number of files: 50\n",
      "../../Dataset/Test-R=10/e15275s3_P27648.7.h5\n",
      "Weights path:\n",
      " ../Models/weights_wwnet_ikik_mc_r10.h5\n",
      "Test path:\n",
      " ../../Dataset/Test-R=10/\n",
      "results path:\n",
      " ../WWnet-ikik-baseline/Track01/12-channel-R=10/\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15275s3_P27648.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e13991s3_P01536.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15865s13_P62464.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14736s3_P55296.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14080s3_P18944.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15493s3_P16896.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15781s13_P96256.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15866s13_P72192.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14618s3_P51200.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15272s3_P07680.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14781s3_P18944.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14091s3_P67584.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14553s5_P44544.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14654s3_P55296.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14655s3_P61952.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16362s3_P07168.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15469s3_P42496.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14155s3_P69120.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15791s4_P11264.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14939s3_P44032.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16649s13_P44544.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e13993s4_P16896.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15768s13_P14848.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15812s13_P51712.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14079s3_P09216.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15390s3_P12288.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16650s13_P54272.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15046s3_P59392.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16031s13_P70656.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15491s3_P01536.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14742s3_P32768.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14078s3_P02048.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15793s3_P25600.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e13992s4_P08704.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e14081s3_P25600.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16031s3_P63488.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15613s4_P52736.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15691s3_P47104.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15790s3_P01536.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15768s3_P07168.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15274s3_P20992.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15865s3_P55296.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15642s3_P48640.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15812s3_P44544.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16649s3_P37376.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16650s3_P47104.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15781s3_P89088.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e16284s3_P25088.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15866s3_P65024.7.h5\n",
      "../WWnet-ikik-baseline/Track01/12-channel-R=10/e15465s3_P13824.7.h5\n"
     ]
    }
   ],
   "source": [
    "test_files = np.asarray(glob.glob(test_path + \"*.h5\"))\n",
    "\n",
    "# Sort by kx - ky size\n",
    "files_sizes = []\n",
    "for ii in test_files:\n",
    "    with h5py.File(ii, 'r') as f:\n",
    "        kspace_test = f['kspace']\n",
    "        files_sizes.append(kspace_test.shape[1]*kspace_test.shape[2]) \n",
    "files_sizes = np.asarray(files_sizes)\n",
    "indexes = np.argsort(files_sizes)\n",
    "test_files = test_files[indexes]\n",
    "\n",
    "if verbose:\n",
    "    print(cascade)\n",
    "    print(\"Domains: \", model_string)\n",
    "    print(\"Number of files:\", len(test_files))\n",
    "    print(test_files[0])\n",
    "    print(\"Weights path:\", weights_path)\n",
    "    print(\"Test path:\", test_path)\n",
    "    print(\"results path:\", results_path)\n",
    "    \n",
    "norm = np.sqrt(218*170)\n",
    "model_exists = False\n",
    "for ii in test_files:\n",
    "\n",
    "    name = ii.split(\"/\")[-1]\n",
    "    # Load data\n",
    "    with h5py.File(ii, 'r') as f:\n",
    "        kspace_test=  np.array(f.get('kspace')).astype(np.float32)[crop[0]:crop[1]]\n",
    "    \n",
    "    kspace_test = kspace_test/norm\n",
    "\n",
    "    Z,H,W,_ = kspace_test.shape\n",
    "    if H*W != (218*170): \n",
    "        model_exists = False\n",
    "\n",
    "    Wpad = (8 -W%8)//2\n",
    "    Hpad = (8 -H%8)//2\n",
    "\n",
    "    if not model_exists:\n",
    "        if cascade == 'unet':\n",
    "            model = fsnet.deep_cascade_unet(depth_str = model_string, H = H, W = W, Hpad = Hpad, Wpad = Wpad, channels = channels) \n",
    "        elif cascade == 'flat':\n",
    "            model = fsnet.deep_cascade_flat_unrolled(depth_str = model_string, H = H,W = W,depth = 14, kshape = (3,3), nf = 116,channels = channels)              \n",
    "\n",
    "        model_exists = True\n",
    "        opt = Adam()\n",
    "        model.compile(loss = 'mse',optimizer = opt)\n",
    "        model.load_weights(weights_path)   \n",
    "\n",
    "    var_sampling_mask = ((kspace_test == 0)).astype(np.float32)\n",
    "    pred = model.predict([ kspace_test, var_sampling_mask],batch_size = batch_size)*norm\n",
    "    pred = pred[:,:,:,::2]+1j*pred[:,:,:,1::2]\n",
    "    pred = np.sqrt((np.abs(pred)**2).sum(axis = -1)) # Root sum of squares\n",
    "\n",
    "    if verbose:\n",
    "        print(os.path.join(results_path,name))\n",
    "    with h5py.File(os.path.join(results_path,name), 'w') as hf:\n",
    "        hf.create_dataset('reconstruction', data=pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
